{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practicum, we will learn about and generate *adversarial examples* for machine learning models, which can be valuable ways to expose potential weak spots in machine learning systems.   We will work from [this paper](https://arxiv.org/abs/1412.6572), which introudced the **Fast Gradient Sign Attack (FGSM)** as a way to fool image classifiers.  Lots of code here is copied verbatim from [this tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method for finding adversarial examples is motivated by the following image:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/fgsm_panda_image.png\">\n",
    "\n",
    "Here, an imperceptible amount of noise is added to a photo of a panda, and the result is that the network classifies the panda as a gibbon!  Our goal is to find these image pairs in a systematic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $J(\\theta,x,y)$ is a *loss function* for an image classification task.  Here, $\\theta$ contains the parameters of our learned model, e.g. the weights of a neural network; $x$ is an image; and $y$ is a label.  The value $J(\\theta,x,y)$ is small when the model correctly predicts label $y$ for image $x$ when parameters $\\theta$ are used.\n",
    "\n",
    "**Question:** Suppose our training data is a set of $N$ image-label pairs $\\{(x_i,y_i)\\}_{i=1}^N$.  Write an optimization problem that reasonably could be used to train a machine learning model that fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we have trained our learning system.  In essence, this means we have chosen a vector of parameters $\\theta$.  With $\\theta$ fixed, we now can search for an adversarial example!\n",
    "\n",
    "Training our model requires the gradient $\\nabla_\\theta J(\\theta,x,y)$.  We now seek interpretation of a different gradient:\n",
    "\n",
    "**Question:** Recalling that the gradient gives the direction of steepest *ascent*, give some intuition for the gradient direction $\\nabla_x J(\\theta,x_i,y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inventors of FBSM make an interesting empirical observation, that for purposes of creating adversarial examples it can be better to use $\\mathrm{sign}(\\nabla_x J(\\theta,x_i,y_i))$ instead of just the gradient; this vector puts $\\pm1$ in every slot corresponding to the sign of $\\nabla_x J$.  They propose a very simple way to get an adversarial example from an image $x_i$:\n",
    "$$\\textrm{adversarial example}_i = x_i + \\varepsilon\\cdot\\mathrm{sign}(\\nabla_x J(\\theta,x_i,y_i)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  Why might we expect that we could find an adversarial example using this formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement this strategy for finding adversarial examples.  First, run the code below to set up a pre-trained neural network for classifying MNIST digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!wget -c https://github.com/ttaranov/mit_adversarial/raw/master/lenet_mnist_model.pth\n",
    "\n",
    "pretrained_model = \"lenet_mnist_model.pth\"\n",
    "use_cuda=False\n",
    "\n",
    "# LeNet Model definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# MNIST Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor()])),batch_size=1, shuffle=True)\n",
    "\n",
    "# Define what device we are using\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement the FGSM attack, in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # TODO:  Implement FGSM attack.  Here, you are given the image (x_i),\n",
    "    # the epsilon value for the attack, and the gradient wrt x of the loss\n",
    "    # J at x_i\n",
    "    \n",
    "    perturbed_image = image # replace me!\n",
    "    \n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to set up a testing tool, which loops over all the test set, evaluates your FGSM attack code, and checks if an adversarial example was found.  Read it carefully to make sure you understand what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    for data, target in test_loader: # Loop over all examples in test set\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # get the index of the max log-probability, the label of the data point\n",
    "        init_pred = output.max(1, keepdim=True)[1] \n",
    "\n",
    "        # If the initial prediction is wrong, don't attack\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect grad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        if final_pred.item() == target.item(): # adversarial example not found\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else: # adversrial example found!\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run our attack!  We have given you a set of $\\varepsilon$ values.  Loop over these values and store accuracies and an adversarial example for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# TODO:  Loop over the epsilons, and append an accuracy and an example output\n",
    "# from the test() function above\n",
    "\n",
    "\n",
    "# Now we can plot the accuracies\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can run the code below to display visual examples of potential adversarial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, most of the \"coding exercise\" above was given to you!  It can be hard to work with libraries like Pytorch in the limited time we have available.  But hopefully you're impressed with the straightforward ability here to generate adversarial attachs on a neural network.\n",
    "\n",
    "Here are several questions to motivate additional experiments:\n",
    "- What is the smallest $\\varepsilon$ you can identify for which this technique yields adversarial examples?\n",
    "- Can you come up with other attack formula than the FGSM attack?  For instance, what happens if you get rid of the \"sign\" aspect of the FGSM formula?\n",
    "\n",
    "For inspiration, check out [this document](https://arxiv.org/pdf/1804.00097.pdf), which lists *many* attack formulas that are not much more complicated than FGSM.  Several of the attacks on page 5 are easily implemented in the framework above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final point of discussion, let's say you're designing a learning system.  Discuss this open-ended question with your colleagues:\n",
    "\n",
    "**Question:** How might you modify training procedures for learning to defend against adversarial attacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
